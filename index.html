<!DOCTYPE html>
<html>
<head>
  <style>
    .carousel .item {
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      padding: 1rem;
      width: 100%;
      max-width: 2000px; /* 可以根据需要调整最大宽度 */
    }
  
    .carousel img {
      width: 100%;
      height: auto;
      max-height: 650px; /* 设置一个合理的最大高度 */
      object-fit: contain;
      aspect-ratio: 4 / 3; /* 设置图片的宽高比，这里以4:3为例 */
    }
  </style>

  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>UniUIR: Considering Underwater Image Restoration as An All-in-One Learner</title>
  <link rel="icon" type="image/x-icon" href="static\images\UniUIR_icon.jpeg">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">UniUIR: <br>Considering Underwater Image Restoration
as An All-in-One Learner</h1>
            
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
              <a href="https://scholar.google.com/citations?user=xDDy-DwAAAAJ" target="_blank">Xu Zhang</a><sup>1</sup>,
              </span>
              
              <span class="author-block">
              <a href="https://scholar.google.com/citations?user=bJjd_kMAAAAJ" target="_blank">Huan Zhang</a><sup>2</sup>,
              </span>
              
              <span class="author-block">
              <a href="https://scholar.google.com/citations?user=z-25fk0AAAAJ" target="_blank">Guoli Wang</a><sup>3</sup>,
              </span>
              
              <span class="author-block">
              <a href="https://scholar.google.com/citations?user=pCY-bikAAAAJ" target="_blank">Qian Zhang</a><sup>3</sup>,
              </span>

              <span class="author-block">
              <a href="https://scholar.google.com/citations?user=BLKHwNwAAAAJ" target="_blank">Lefei Zhang</a><sup>1,📧</sup>,
              </span>

              <span class="author-block">
              <a href="https://scholar.google.com.hk/citations?user=Shy1gnMAAAAJ&hl=zh-CN" target="_blank">Bo Du</a><sup>1</sup>
              </span>
            </div>
            
            <!-- 单位 + 接收信息 -->
            <div class="is-size-5 publication-authors" style="margin-top: 0.5rem;">
              Wuhan University<sup>1</sup><br>
              Guangdong University of Technology<sup>2</sup><br>
              Horizon Robotics<sup>3</sup><br>
              <i><b>Accepted by IEEE TIP 2025</b></i>
            </div>

            <!-- 通讯作者标注 -->
            <div style="margin-top: 0.3rem;">
              <small><sup>📧</sup> Corresponding Author</small>
            </div>

            <!-- 链接按钮 -->
            <div class="publication-links" style="margin-top: 1rem;">
                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/House-yuyu/UniUIR" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2501.12981" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

                <!-- PDF Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2501.12981" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
                      
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>




<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video> -->
      <div class="item">
        <!-- Your image here -->
        <center><img src="fig\UIR_motivation.png" alt="MY ALT TEXT" width="600px" height="600px"/></center>
        <h2 class="subtitle has-text-lefted">
        The figures present a subjective statistical analysis of the predominant distortions in the UIEB dataset. Although each image may exhibit multiple distortions, for simplified classification, each is categorized by its most visually prominent distortion.
        </h2>
      </div>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Existing underwater image restoration (UIR) methods generally only handle color distortion or jointly address color and haze issues, but they often overlook the more complex
degradations that can occur in underwater scenes. To address this limitation, we propose a Universal Underwater Image Restoration method, termed as UniUIR, considering the complex scenario of
real-world underwater mixed distortions as an all-in-one manner. To disentangle degradation-specific effects and capture their inter-correlations, we propose the Mamba Mixture-of-Experts
module (MMoEM). Each expert specializes in distinct aspects of degradation, while gating mechanism dynamically routes features to appropriate experts. This design enables collaborative
prior extraction and preserves global context, all within linear computational complexity. Building upon this foundation, to enhance degradation representation and address the task conflicts
that arise when handling multiple types of degradation, we introduce the spatial-frequency prior generator. This module extracts degradation prior information in both spatial and
frequency domains, and adaptively selects the most appropriate task-specific prompts based on image content, thereby improving the accuracy of image restoration. Finally, to more effectively
address complex, region-dependent distortions in UIR task, we incorporate depth information derived from a large-scale pre-trained depth prediction model, thereby enabling the network
to perceive and leverage depth variations across different image regions to handle localized degradation. Extensive experiments demonstrate that UniUIR can produce more attractive results
across qualitative and quantitative comparisons, and shows strong generalization than state-of-the-art methods.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->





<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="fig\overall.png" alt="MY ALT TEXT" width="600px" height="450px"/>
        <h2 class="subtitle has-text-lefted">
          Overview of the proposed UniUIR.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="fig\MMoE-UIR.png" alt="MY ALT TEXT" width="600px" height="450px"/>
        <h2 class="subtitle has-text-lefted">
        The detailed structure of the proposed MMoE-UIR.
        </h2>
      </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->



<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="fig\vis1.png" alt="MY ALT TEXT" width="400px" height="300px"/>
        <h2 class="subtitle has-text-centered">
          Visual comparison of restored results for the T90 test set.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="fig\vis_noref.png" alt="MY ALT TEXT" width="400px" height="300px"/>
        <h2 class="subtitle has-text-centered">
          Visual comparison of restoration results on the non-reference underwater image datasets.
        </h2>
      </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->



<!-- Image carousel -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="fig\vis_LLIE.png" alt="MY ALT TEXT" width="400px" height="300px"/>
        <h2 class="subtitle has-text-centered">
          Qualitative results on LOL-v1 (top), LOL-v2-real (middle), and BAID (bottom).
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="fig\vis_sam.png" alt="MY ALT TEXT" width="400px" height="300px"/>
        <h2 class="subtitle has-text-centered">
          The depth maps and semantic segmentation results generated by Depth Anything V2 and the Segment Anything Model.
        </h2>
      </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->






<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{zhang2025uniuir,
  title={UniUIR: Considering Underwater Image Restoration as An All-in-One Learner},
  author={Zhang, Xu and Zhang, Huan and Wang, Guoli and Zhang, Qian and Zhang, Lefei and Du, Bo},
  journal={arXiv preprint arXiv:2501.12981},
  year={2025}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- Default Statcounter code for Perceive-IR
https://house-yuyu.github.io/Perceive-IR -->
<script type="text/javascript">
  var sc_project=13053453; 
  var sc_invisible=0; 
  var sc_security="2b6ddbea"; 
  var scJsHost = "https://";
  document.write("<sc"+"ript type='text/javascript' src='" +
  scJsHost+
  "statcounter.com/counter/counter.js'></"+"script>");
  </script>
  <noscript><div class="statcounter"><a title="Web Analytics"
  href="https://statcounter.com/" target="_blank"><img
  class="statcounter"
  src="https://c.statcounter.com/13053453/0/2b6ddbea/0/"
  alt="Web Analytics"
  referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
  <!-- End of Statcounter Code -->

  </body>
  </html>
